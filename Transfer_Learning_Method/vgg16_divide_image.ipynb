{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./for_transfer_learning/model/transfer_learn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/opt/conda/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.1296196]]\n",
      "[[1.4846036]]\n",
      "[[1.2408715]]\n",
      "[[1.5542499]]\n",
      "[[0.9291678]]\n",
      "[[1.3608376]]\n",
      "[[0.9692491]]\n",
      "[[1.0595759]]\n",
      "[[2.0297747]]\n",
      "[[1.3836969]]\n",
      "[[1.4716669]]\n",
      "[[1.7066393]]\n",
      "//////////////////\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "def divide_img():\n",
    "    ra_dataframe  = pd.read_csv('./rough_data.csv', header=None)\n",
    "    files         = glob.glob('./right/*')\n",
    "    files.extend(glob.glob('./left/*'))\n",
    "    process_dataframe = pd.DataFrame(columns=['path','value','class'])\n",
    "    print(files)\n",
    "    for file_path in files:\n",
    "        left,right = file_path.split(\"/\", 1)\n",
    "        dire,right = right.split(\"\\\\\", 1)\n",
    "        num,right  = right.split(\"_\",1)\n",
    "\n",
    "        row        = 0\n",
    "        col        = 0\n",
    "        if dire == 'left':\n",
    "            col = 0\n",
    "        else:\n",
    "            col = 1\n",
    "        row = int(num) - 1\n",
    "        ra_value = ra_dataframe.iloc[row,col]\n",
    "        if float(ra_value) < 1.5:\n",
    "            os.makedirs('./rough1.0-1.5', exist_ok=True)\n",
    "            #divide image to six images\n",
    "\n",
    "            image = cv2.imread(file_path)\n",
    "            image1 = image[:256,:256]\n",
    "            image2 = image[:256,256:512]\n",
    "            image3 = image[:256,488:744]\n",
    "            image4 = image[224:480,:256]\n",
    "            image5 = image[224:480,256:512]\n",
    "            image6 = image[224:480,488:744]\n",
    "\n",
    "            f_1    = np.fft.fft2(image1)\n",
    "            f_2    = np.fft.fft2(image2)\n",
    "            f_3    = np.fft.fft2(image3)\n",
    "            f_4    = np.fft.fft2(image4)\n",
    "            f_5    = np.fft.fft2(image5)\n",
    "            f_6    = np.fft.fft2(image6)\n",
    "    \n",
    "            fshift_1 = np.fft.fftshift(f_1)\n",
    "            fshift_2 = np.fft.fftshift(f_2)\n",
    "            fshift_3 = np.fft.fftshift(f_3)\n",
    "            fshift_4 = np.fft.fftshift(f_4)\n",
    "            fshift_5 = np.fft.fftshift(f_5)\n",
    "            fshift_6 = np.fft.fftshift(f_6)\n",
    "            \n",
    "            magnitude_spectrum_1 = 20 * np.log(np.abs(fshift_1))\n",
    "            magnitude_spectrum_2 = 20 * np.log(np.abs(fshift_2))\n",
    "            magnitude_spectrum_3 = 20 * np.log(np.abs(fshift_3))\n",
    "            magnitude_spectrum_4 = 20 * np.log(np.abs(fshift_4))\n",
    "            magnitude_spectrum_5 = 20 * np.log(np.abs(fshift_5))\n",
    "            magnitude_spectrum_6 = 20 * np.log(np.abs(fshift_6))\n",
    "\n",
    "            magnitude_spectrum_1 = cv2.normalize(magnitude_spectrum_1,  magnitude_spectrum_1, 0, 255, cv2.NORM_MINMAX)\n",
    "            magnitude_spectrum_2 = cv2.normalize(magnitude_spectrum_2,  magnitude_spectrum_2, 0, 255, cv2.NORM_MINMAX)\n",
    "            magnitude_spectrum_3 = cv2.normalize(magnitude_spectrum_3,  magnitude_spectrum_3, 0, 255, cv2.NORM_MINMAX)\n",
    "            magnitude_spectrum_4 = cv2.normalize(magnitude_spectrum_4,  magnitude_spectrum_4, 0, 255, cv2.NORM_MINMAX)\n",
    "            magnitude_spectrum_5 = cv2.normalize(magnitude_spectrum_5,  magnitude_spectrum_5, 0, 255, cv2.NORM_MINMAX)\n",
    "            magnitude_spectrum_6 = cv2.normalize(magnitude_spectrum_6,  magnitude_spectrum_6, 0, 255, cv2.NORM_MINMAX)\n",
    "            \n",
    "            store_path = './rough1.0-1.5/' + str(row+1) + '_' + dire + '_1' + '.jpg'\n",
    "            cv2.imwrite(store_path, image1)\n",
    "            store_path = './rough1.0-1.5/' + str(row+1) + '_' + dire + '_2' + '.jpg'\n",
    "            cv2.imwrite(store_path, image2)\n",
    "            store_path = './rough1.0-1.5/' + str(row+1) + '_' + dire + '_3' + '.jpg'\n",
    "            cv2.imwrite(store_path, image3)\n",
    "            store_path = './rough1.0-1.5/' + str(row+1) + '_' + dire + '_4' + '.jpg'\n",
    "            cv2.imwrite(store_path, image4)\n",
    "            store_path = './rough1.0-1.5/' + str(row+1) + '_' + dire + '_5' + '.jpg'\n",
    "            cv2.imwrite(store_path, image5)\n",
    "            store_path = './rough1.0-1.5/' + str(row+1) + '_' + dire + '_6' + '.jpg'\n",
    "            cv2.imwrite(store_path, image6)\n",
    "\n",
    "        elif float(ra_value) >= 1.5 and float(ra_value) < 2.0:\n",
    "            os.makedirs('./rough1.5-2.0/', exist_ok=True)\n",
    "            image = cv2.imread(file_path)\n",
    "            image1 = image[:256,:256]\n",
    "            image2 = image[:256,256:512]\n",
    "            image3 = image[:256,488:744]\n",
    "            image4 = image[224:480,:256]\n",
    "            image5 = image[224:480,256:512]\n",
    "            image6 = image[224:480,488:744]\n",
    "\n",
    "            f_1    = np.fft.fft2(image1)\n",
    "            f_2    = np.fft.fft2(image2)\n",
    "            f_3    = np.fft.fft2(image3)\n",
    "            f_4    = np.fft.fft2(image4)\n",
    "            f_5    = np.fft.fft2(image5)\n",
    "            f_6    = np.fft.fft2(image6)\n",
    "            \n",
    "            fshift_1 = np.fft.fftshift(f_1)\n",
    "            fshift_2 = np.fft.fftshift(f_2)\n",
    "            fshift_3 = np.fft.fftshift(f_3)\n",
    "            fshift_4 = np.fft.fftshift(f_4)\n",
    "            fshift_5 = np.fft.fftshift(f_5)\n",
    "            fshift_6 = np.fft.fftshift(f_6)\n",
    "            \n",
    "            magnitude_spectrum_1 = 20 * np.log(np.abs(fshift_1))\n",
    "            magnitude_spectrum_2 = 20 * np.log(np.abs(fshift_2))\n",
    "            magnitude_spectrum_3 = 20 * np.log(np.abs(fshift_3))\n",
    "            magnitude_spectrum_4 = 20 * np.log(np.abs(fshift_4))\n",
    "            magnitude_spectrum_5 = 20 * np.log(np.abs(fshift_5))\n",
    "            magnitude_spectrum_6 = 20 * np.log(np.abs(fshift_6))\n",
    "            \n",
    "            magnitude_spectrum_1 = cv2.normalize(magnitude_spectrum_1,  magnitude_spectrum_1, 0, 255, cv2.NORM_MINMAX)\n",
    "            magnitude_spectrum_2 = cv2.normalize(magnitude_spectrum_2,  magnitude_spectrum_2, 0, 255, cv2.NORM_MINMAX)\n",
    "            magnitude_spectrum_3 = cv2.normalize(magnitude_spectrum_3,  magnitude_spectrum_3, 0, 255, cv2.NORM_MINMAX)\n",
    "            magnitude_spectrum_4 = cv2.normalize(magnitude_spectrum_4,  magnitude_spectrum_4, 0, 255, cv2.NORM_MINMAX)\n",
    "            magnitude_spectrum_5 = cv2.normalize(magnitude_spectrum_5,  magnitude_spectrum_5, 0, 255, cv2.NORM_MINMAX)\n",
    "            magnitude_spectrum_6 = cv2.normalize(magnitude_spectrum_6,  magnitude_spectrum_6, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "            store_path = './rough1.5-2.0/' + str(row+1) + '_' + dire + '_1' + '.jpg'\n",
    "            cv2.imwrite(store_path, image1)\n",
    "            store_path = './rough1.5-2.0/' + str(row+1) + '_' + dire + '_2' + '.jpg'\n",
    "            cv2.imwrite(store_path, image2)\n",
    "            store_path = './rough1.5-2.0/' + str(row+1) + '_' + dire + '_3' + '.jpg'\n",
    "            cv2.imwrite(store_path, image3)\n",
    "            store_path = './rough1.5-2.0/' + str(row+1) + '_' + dire + '_4' + '.jpg'\n",
    "            cv2.imwrite(store_path, image4)\n",
    "            store_path = './rough1.5-2.0/' + str(row+1) + '_' + dire + '_5' + '.jpg'\n",
    "            cv2.imwrite(store_path, image5)\n",
    "            store_path = './rough1.5-2.0/' + str(row+1) + '_' + dire + '_6' + '.jpg'\n",
    "            cv2.imwrite(store_path, image6)\n",
    "\n",
    "        elif float(ra_value) >= 2.0:\n",
    "            os.makedirs('./rough2.0-2.5/', exist_ok=True)\n",
    "            image = cv2.imread(file_path)\n",
    "            image1 = image[:256,:256]\n",
    "            image2 = image[:256,256:512]\n",
    "            image3 = image[:256,488:744]\n",
    "            image4 = image[224:480,:256]\n",
    "            image5 = image[224:480,256:512]\n",
    "            image6 = image[224:480,488:744]\n",
    "\n",
    "            f_1    = np.fft.fft2(image1)\n",
    "            f_2    = np.fft.fft2(image2)\n",
    "            f_3    = np.fft.fft2(image3)\n",
    "            f_4    = np.fft.fft2(image4)\n",
    "            f_5    = np.fft.fft2(image5)\n",
    "            f_6    = np.fft.fft2(image6)\n",
    "\n",
    "            fshift_1 = np.fft.fftshift(f_1)\n",
    "            fshift_2 = np.fft.fftshift(f_2)\n",
    "            fshift_3 = np.fft.fftshift(f_3)\n",
    "            fshift_4 = np.fft.fftshift(f_4)\n",
    "            fshift_5 = np.fft.fftshift(f_5)\n",
    "            fshift_6 = np.fft.fftshift(f_6)\n",
    "\n",
    "            magnitude_spectrum_1 = 20 * np.log(np.abs(fshift_1))\n",
    "            magnitude_spectrum_2 = 20 * np.log(np.abs(fshift_2))\n",
    "            magnitude_spectrum_3 = 20 * np.log(np.abs(fshift_3))\n",
    "            magnitude_spectrum_4 = 20 * np.log(np.abs(fshift_4))\n",
    "            magnitude_spectrum_5 = 20 * np.log(np.abs(fshift_5))\n",
    "            magnitude_spectrum_6 = 20 * np.log(np.abs(fshift_6))\n",
    "\n",
    "            magnitude_spectrum_1 = cv2.normalize(magnitude_spectrum_1,  magnitude_spectrum_1, 0, 255, cv2.NORM_MINMAX)\n",
    "            magnitude_spectrum_2 = cv2.normalize(magnitude_spectrum_2,  magnitude_spectrum_2, 0, 255, cv2.NORM_MINMAX)\n",
    "            magnitude_spectrum_3 = cv2.normalize(magnitude_spectrum_3,  magnitude_spectrum_3, 0, 255, cv2.NORM_MINMAX)\n",
    "            magnitude_spectrum_4 = cv2.normalize(magnitude_spectrum_4,  magnitude_spectrum_4, 0, 255, cv2.NORM_MINMAX)\n",
    "            magnitude_spectrum_5 = cv2.normalize(magnitude_spectrum_5,  magnitude_spectrum_5, 0, 255, cv2.NORM_MINMAX)\n",
    "            magnitude_spectrum_6 = cv2.normalize(magnitude_spectrum_6,  magnitude_spectrum_6, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "            store_path = './rough2.0-2.5/' + str(row+1) + '_' + dire + '_1' + '.jpg'\n",
    "            cv2.imwrite(store_path, image1)\n",
    "            store_path = './rough2.0-2.5/' + str(row+1) + '_' + dire + '_2' + '.jpg'\n",
    "            cv2.imwrite(store_path, image2)\n",
    "            store_path = './rough2.0-2.5/' + str(row+1) + '_' + dire + '_3' + '.jpg'\n",
    "            cv2.imwrite(store_path,image3)\n",
    "            store_path = './rough2.0-2.5/' + str(row+1) + '_' + dire + '_4' + '.jpg'\n",
    "            cv2.imwrite(store_path,image4)\n",
    "            store_path = './rough2.0-2.5/' + str(row+1) + '_' + dire + '_5' + '.jpg'\n",
    "            cv2.imwrite(store_path, image5)\n",
    "            store_path = './rough2.0-2.5/' + str(row+1) + '_' + dire + '_6' + '.jpg'\n",
    "            cv2.imwrite(store_path, image6)\n",
    "        \n",
    "def load_img(path):\n",
    "    img = skimage.io.imread(path)\n",
    "    img = img / 255.0\n",
    "    short_edge = min(img.shape[:2])\n",
    "    yy = int((img.shape[0] - short_edge) / 2)\n",
    "    xx = int((img.shape[1] - short_edge) / 2)\n",
    "    crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n",
    "    resized_img = skimage.transform.resize(crop_img, (224, 224))[None, :, :, :]   # shape [1, 224, 224, 3]\n",
    "    return resized_img\n",
    "\n",
    "def load_data():\n",
    "    imgs   = {'rough1.0-1.5': [] , 'rough1.5-2.0': [],'rough2.0-2.5': []}\n",
    "    answer = {'rough1.0-1.5': [] , 'rough1.5-2.0': [], 'rough2.0-2.5': []}\n",
    "    #format 6_right_1\n",
    "    for k in imgs.keys():\n",
    "        dir = './' + k + '/'\n",
    "        for file in os.listdir(dir):\n",
    "            if not file.lower().endswith('.jpg'):\n",
    "                continue\n",
    "            try:\n",
    "                resized_img = load_img(os.path.join(dir, file))\n",
    "            except OSError:\n",
    "                continue\n",
    "            imgs[k].append(resized_img)\n",
    "            if k == 'rough1.0-1.5':\n",
    "                answer[k].append(0.0)\n",
    "            elif k == 'rough1.5-2.0':\n",
    "                answer[k].append(1.0)\n",
    "            elif k == 'rough2.0-2.5':\n",
    "                answer[k].append(2.0)\n",
    "    \n",
    "    return imgs['rough1.0-1.5'], imgs['rough1.5-2.0'], imgs['rough2.0-2.5'],answer['rough1.0-1.5'],answer['rough1.5-2.0'],answer['rough2.0-2.5']\n",
    "\n",
    "class Vgg16:\n",
    "    vgg_mean = [103.939, 116.779, 123.68]\n",
    "\n",
    "    def __init__(self, vgg16_npy_path=None, restore_from=None):\n",
    "        # pre-trained parameters\n",
    "        try:\n",
    "            self.data_dict = np.load(vgg16_npy_path, encoding='latin1').item()\n",
    "        except FileNotFoundError:\n",
    "            print('Please download VGG16 parameters from here https://mega.nz/#!YU1FWJrA!O1ywiCS2IiOlUCtCpI6HTJOMrneN-Qdv3ywQP5poecM\\nOr from my Baidu Cloud: https://pan.baidu.com/s/1Spps1Wy0bvrQHH2IMkRfpg')\n",
    "\n",
    "        self.tfx = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "        self.tfy = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        # Convert RGB to BGR\n",
    "        red, green, blue = tf.split(axis=3, num_or_size_splits=3, value=self.tfx * 255.0)\n",
    "        bgr = tf.concat(axis=3, values=[\n",
    "            blue - self.vgg_mean[0],\n",
    "            green - self.vgg_mean[1],\n",
    "            red - self.vgg_mean[2],\n",
    "        ])\n",
    "\n",
    "        # pre-trained VGG layers are fixed in fine-tune\n",
    "        conv1_1 = self.conv_layer(bgr, \"conv1_1\")\n",
    "        conv1_2 = self.conv_layer(conv1_1, \"conv1_2\")\n",
    "        pool1 = self.max_pool(conv1_2, 'pool1')\n",
    "\n",
    "        conv2_1 = self.conv_layer(pool1, \"conv2_1\")\n",
    "        conv2_2 = self.conv_layer(conv2_1, \"conv2_2\")\n",
    "        pool2 = self.max_pool(conv2_2, 'pool2')\n",
    "\n",
    "        conv3_1 = self.conv_layer(pool2, \"conv3_1\")\n",
    "        conv3_2 = self.conv_layer(conv3_1, \"conv3_2\")\n",
    "        conv3_3 = self.conv_layer(conv3_2, \"conv3_3\")\n",
    "        pool3   = self.max_pool(conv3_3, 'pool3')\n",
    "\n",
    "        conv4_1 = self.conv_layer(pool3, \"conv4_1\")\n",
    "        conv4_2 = self.conv_layer(conv4_1, \"conv4_2\")\n",
    "        conv4_3 = self.conv_layer(conv4_2, \"conv4_3\")\n",
    "        pool4   = self.max_pool(conv4_3, 'pool4')\n",
    "\n",
    "        conv5_1 = self.conv_layer(pool4, \"conv5_1\")\n",
    "        conv5_2 = self.conv_layer(conv5_1, \"conv5_2\")\n",
    "        conv5_3 = self.conv_layer(conv5_2, \"conv5_3\")\n",
    "        pool5 = self.max_pool(conv5_3, 'pool5')\n",
    "\n",
    "        # detach original VGG fc layers and\n",
    "        # reconstruct your own fc layers serve for your own purpose\n",
    "        self.flatten = tf.reshape(pool5, [-1, 7*7*512])\n",
    "        self.fc6 = tf.layers.dense(self.flatten, 256, tf.nn.relu, name='fc6')\n",
    "        self.out = tf.layers.dense(self.fc6, 1, name='out')\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        if restore_from:\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(self.sess, restore_from)\n",
    "        else:   # training graph\n",
    "            self.loss = tf.losses.mean_squared_error(labels=self.tfy, predictions=self.out)\n",
    "            self.train_op = tf.train.RMSPropOptimizer(0.001).minimize(self.loss)\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def max_pool(self, bottom, name):\n",
    "        return tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    def conv_layer(self, bottom, name):\n",
    "        with tf.variable_scope(name):   # CNN's filter is constant, NOT Variable that can be trained\n",
    "            conv = tf.nn.conv2d(bottom, self.data_dict[name][0], [1, 1, 1, 1], padding='SAME')\n",
    "            lout = tf.nn.relu(tf.nn.bias_add(conv, self.data_dict[name][1]))\n",
    "            return lout\n",
    "\n",
    "    def train(self, x, y):\n",
    "        loss, _ = self.sess.run([self.loss, self.train_op], {self.tfx: x, self.tfy: y})\n",
    "        return loss\n",
    "\n",
    "    def predict(self, paths):\n",
    "        fig, axs = plt.subplots(1, 2)\n",
    "        imgs   = {'test': [] }\n",
    "        for k in imgs.keys():\n",
    "            dir = './' + k + '/'\n",
    "            for file in os.listdir(dir):\n",
    "                if not file.lower().endswith('.jpg'):\n",
    "                    continue\n",
    "                resized_img = load_img(os.path.join(dir, file))\n",
    "                length = self.sess.run(self.out, {self.tfx: resized_img})\n",
    "                print(length)\n",
    "        \n",
    "    def save(self, path='./for_transfer_learning/model/transfer_learn'):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self.sess, path, write_meta_graph=False)\n",
    "\n",
    "def train():\n",
    "    rough0_x, rough1_x, rough2_x , rough0_y, rough1_y, rough2_y = load_data()\n",
    "    # plot fake length distribution\n",
    "    rough0_y = np.resize(np.array(rough0_y),(len(rough0_y),1))\n",
    "    rough1_y = np.resize(np.array(rough1_y),(len(rough1_y),1))\n",
    "    rough2_y = np.resize(np.array(rough2_y),(len(rough2_y),1))\n",
    "    plt.hist(rough0_y, bins=20, label='rough0')\n",
    "    plt.hist(rough1_y, bins=10, label='rough1')\n",
    "    plt.hist(rough2_y, bins=10, label='rough2')\n",
    "    plt.legend()\n",
    "    plt.xlabel('length')\n",
    "    plt.show()\n",
    "    \n",
    "    xs = np.concatenate(rough0_x+rough1_x+rough2_x,axis=0)\n",
    "    ys = np.concatenate((rough0_y,rough1_y,rough2_y), axis=0)\n",
    "    vgg = Vgg16(vgg16_npy_path='./for_transfer_learning/vgg16.npy')\n",
    "    print('Net built')\n",
    "    for i in range(1000):\n",
    "        b_idx = np.random.randint(0, len(xs), 50)\n",
    "        train_loss = vgg.train(xs[b_idx], ys[b_idx])\n",
    "        print(i, 'train loss: ', train_loss)\n",
    "\n",
    "    vgg.save('./for_transfer_learning/model/transfer_learn')\n",
    "    \n",
    "def eval():\n",
    "    vgg = Vgg16(vgg16_npy_path='./for_transfer_learning/vgg16.npy',\n",
    "                restore_from='./for_transfer_learning/model/transfer_learn')\n",
    "    vgg.predict(\n",
    "        ['./rough1.0-1.5/6_right_6.jpg', './rough1.5-2.0/3_right_5.jpg','./rough1.0-1.5/14_right_6.jpg','./rough2.0-2.5/18_right_1.jpg'])\n",
    "if __name__ == '__main__':\n",
    "    #train()\n",
    "    eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
